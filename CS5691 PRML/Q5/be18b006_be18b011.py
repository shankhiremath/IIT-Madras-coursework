# -*- coding: utf-8 -*-
"""BE18B006_BE18B011

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cipx9A3juVov-n4E795afinfKgQQspmP

# General Instructions to students:

1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.
    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)
    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)
    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)
    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)
    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)
    
2. You are not allowed to insert new cells in the submitted notebook.

3. You are not allowed to **import** any extra packages.

4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.

5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. 

6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.

7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with "run all" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.

8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.

9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :"X_train, Y_train, X_test, Y_test". In that order. The meaning of the 4 arrays can be easily inferred from their names.

10. All plots must be labelled properly, all tables must have rows and columns named properly.

11. Change the name of file with your roll no.
"""

# Cell type : CodeRead

import numpy as np
import matplotlib.pyplot as plt

"""**Cell type : TextRead**

# Problem 4: Learning Binary Bayes Classifiers from data with Max. Likelihood 

Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. 

BayesA) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, I)$ and  $X|Y=1 \sim \mathcal{N}(\mu_+, I)$. *(Same known covariance)*

BayesB) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, \Sigma)$ and $X|Y=1 \sim \mathcal{N}(\mu_+, \Sigma)$ *(Same unknown covariance)*

BayesC) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, \Sigma_-)$ and $X|Y=1 \sim \mathcal{N}(\mu_+, \Sigma_+)$ *(different unknown covariance)*


"""

# Cell type : CodeWrite

def function_for_A(X_train, Y_train, X_test):
    """ Give prediction for test instance using assumption BayesA.

    Arguments:
    X_train: numpy array of shape (n,d)
    Y_train: +1/-1 numpy array of shape (n,)
    X_test : numpy array of shape (m,d)

    Returns:
    Y_test_pred : +1/-1 numpy array of shape (m,)
    
    """
    n,d = X_train.shape
    m,d = X_test.shape
    X_train_pos = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==1])
    X_train_neg = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==-1])
    p_pos = len(X_train_pos)/n
    p_neg = len(X_train_neg)/n
    mu_neg = X_train_neg.sum(axis=0)/len(X_train_neg)
    mu_pos = X_train_pos.sum(axis=0)/len(X_train_pos)
    probabs = []
    #calculating posterior probability of new data
    probab = lambda x: (p_pos*np.exp(-0.5*np.dot(np.subtract(x,mu_pos),np.subtract(x,mu_pos))))/(p_pos*np.exp(-0.5*np.dot(np.subtract(x,mu_pos),np.subtract(x,mu_pos)))+p_neg*np.exp(-0.5*np.dot(np.subtract(x,mu_neg),np.subtract(x,mu_neg))))
    w = np.subtract(mu_pos,mu_neg)
    b = -0.5*np.dot(mu_pos,mu_pos)+0.5*np.dot(mu_neg,mu_neg)+np.log(p_pos/p_neg)
    test_y = []
    for i in range(m):
      if probab(X_test[i,:])>=0.5:
        test_y.append(1)
        probabs.append(probab(X_test[i,:]))
      else:
        test_y.append(-1)
        probabs.append(probab(X_test[i,:]))

    return test_y,w,b,probabs


    
def function_for_B(X_train, Y_train, X_test):
    """ Give prediction for test instance using assumption BayesB.

    Arguments:
    X_train: numpy array of shape (n,d)
    Y_train: +1/-1 numpy array of shape (n,)
    X_test : numpy array of shape (m,d)

    Returns:
    Y_test_pred : +1/-1 numpy array of shape (m,)
    
    """
    n,d = X_train.shape
    m,d = X_test.shape
    X_train_pos = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==1])
    X_train_neg = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==-1])
    p_pos = len(X_train_pos)/n
    p_neg = len(X_train_neg)/n
    mu_neg = X_train_neg.sum(axis=0)/len(X_train_neg)
    mu_pos = X_train_pos.sum(axis=0)/len(X_train_pos)
    mu = X_train.sum(axis=0)/n

    cov_matrix = np.zeros((d,d))

    for i in range(n):
      cov = np.matmul(np.expand_dims(X_train[i,:],axis=1),np.expand_dims(X_train[i,:],axis=0))
      cov_matrix = np.add(cov_matrix,cov)
    cov_matrix = cov_matrix/n
    inv_cov = np.linalg.inv(cov_matrix)

    func = lambda x,mu: np.matmul(np.matmul(np.expand_dims(np.subtract(x,mu),axis=0),inv_cov),np.expand_dims(np.subtract(x,mu),axis=1))
    func2 = lambda x: np.matmul(np.matmul(np.expand_dims(x,axis=0),inv_cov),np.expand_dims(x,axis=1))

    probab = lambda x: (p_pos*np.exp(-0.5*func(x,mu_pos)))/(p_pos*np.exp(-0.5*func(x,mu_pos))+p_neg*np.exp(-0.5*func(x,mu_neg)))
    w =np.matmul(inv_cov,np.subtract(mu_pos,mu_neg))
    b = -0.5*func2(mu_pos)+0.5*func2(mu_neg)+np.log(p_pos/p_neg)
    test_y = []
    probabs = []
    for i in range(m):
      if probab(X_test[i,:])>=0.5:
        test_y.append(1)
        probabs.append(probab(X_test[i,:]))
      else:
        test_y.append(-1)
        probabs.append(probab(X_test[i,:]))

    return test_y,w,b[0,0],probabs



def function_for_C(X_train, Y_train, X_test):
    """ Give prediction for test instance using assumption BayesC.

    Arguments:
    X_train: numpy array of shape (n,d)
    Y_train: +1/-1 numpy array of shape (n,)
    X_test : numpy array of shape (m,d)

    Returns:
    Y_test_pred : +1/-1 numpy array of shape (m,)
    
    """
    n,d = X_train.shape
    m,d = X_test.shape
    X_train_pos = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==1])
    X_train_neg = np.array([X_train[i,:] for i in range(len(X_train)) if Y_train[i]==-1])
    p_pos = len(X_train_pos)/n
    p_neg = len(X_train_neg)/n    
    mu_neg = X_train_neg.sum(axis=0)/X_train_neg.shape[0]
    mu_pos = X_train_pos.sum(axis=0)/X_train_pos.shape[0]

    cov_matrix_pos = np.zeros((d,d))
    cov_matrix_neg = np.zeros((d,d))
    
    for i in range(len(X_train_pos)):
      cov = np.matmul(np.expand_dims(X_train_pos[i,:],axis=1),np.expand_dims(X_train_pos[i,:],axis=0))
      cov_matrix_pos = np.add(cov_matrix_pos,cov)

    for i in range(len(X_train_neg)):
      cov = np.matmul(np.expand_dims(X_train_neg[i,:],axis=1),np.expand_dims(X_train_neg[i,:],axis=0))
      cov_matrix_neg = np.add(cov_matrix_neg,cov)

    cov_matrix_pos = np.divide(cov_matrix_pos,X_train_pos.shape[0])
    cov_matrix_neg = np.divide(cov_matrix_neg,X_train_neg.shape[0])

    inv_cov_pos = np.linalg.inv(cov_matrix_pos)
    inv_cov_neg = np.linalg.inv(cov_matrix_neg)
    det_pos = np.absolute(np.linalg.det(cov_matrix_pos))
    det_neg = np.absolute(np.linalg.det(cov_matrix_neg))

    func = lambda x,mu,c: np.matmul(np.matmul(np.expand_dims(np.subtract(x,mu),axis=0),c),np.expand_dims(np.subtract(x,mu),axis=1))
    func2 = lambda x,c: np.matmul(np.matmul(np.expand_dims(x,axis=0),c),np.expand_dims(x,axis=1))
    probab = lambda x: np.divide(p_pos*np.exp(-0.5*func(x,mu_pos,inv_cov_pos)),np.sqrt(det_pos))/(np.divide(p_pos*np.exp(-0.5*func(x,mu_pos,inv_cov_pos)),np.sqrt(det_pos))+np.divide(p_neg*np.exp(-0.5*func(x,mu_neg,inv_cov_neg)),np.sqrt(det_neg)))
    w = np.subtract(np.matmul(inv_cov_pos,mu_pos),np.matmul(inv_cov_neg,mu_neg))
    b = -0.5*func2(mu_pos,inv_cov_pos)+0.5*func2(mu_neg,inv_cov_neg)+np.log((p_pos*np.sqrt(det_neg))/(p_neg*np.sqrt(det_pos)))
    test_y = []
    probabs = []
    for i in range(m):
      if probab(X_test[i,:])>=0.5:
        test_y.append(1)
        probabs.append(probab(X_test[i,:]))
      else:
        test_y.append(-1)
        probabs.append(probab(X_test[i,:]))

    return test_y,w,b[0,0],probabs

# Cell type : Convenience

# Testing the functions above

# To students: You may use the example here for testing syntax issues 
# with your functions, and also as a sanity check. But the final evaluation
# will be done for different inputs to the functions. (So you can't just 
# solve the problem for this one example given below.) 
# try to remove everything or comment out your lines before submitting.


X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])
X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])
X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)
Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))
X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])
X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])
X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)
Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))

Y_pred_test_1a,wa,ba,_ = function_for_A(X_train, Y_train, X_test)
Y_pred_test_1b,wb,bb,_ = function_for_B(X_train, Y_train, X_test)
Y_pred_test_1c,wc,bc,_ = function_for_C(X_train, Y_train, X_test)
sum(np.array(Y_pred_test_1c)==1)
print(bc)

"""**Cell type : TextRead**

# Problem 4

#### 4a) Run the above three algorithms (BayesA,B and C), for the two datasets given (datasetA.npz, datasetB.npz) in the cell below.
#### In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 2 datasets = 6 plots) on a 2d plot (color the positively classified area light green, and negatively classified area light red). Add the training data points also on the plot. Plots to be organised into 2 as follows: One plot for each dataset, with 3 subplots in each for the three classifiers. Label the 6 plots appropriately.




"""

# Cell type : CodeWrite
# write the code for loading the data, running the three algos, and plotting here. 
# (Use the functions written previously.)
#from google.colab import drive
#drive.mount('/content/drive')

#%cd /content/drive/My\ Drive/Colab\ Notebooks/PRML\ Assignment/Assignment1/
datasetA = np.load('datasetA.npz',mmap_mode='r')
datasetB = np.load('datasetB.npz',mmap_mode='r')

def classify(X_train, Y_train,X_test,Y_test,function,case):

  min1, max1 = X_test[:, 0].min()-1, X_test[:, 0].max()+1
  min2, max2 = X_test[:, 1].min()-1, X_test[:, 1].max()+1
  x1_scale = np.arange(min1, max1, 0.1)
  x2_scale = np.arange(min2, max2, 0.1)
  x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)

  # flatten each grid to a vector
  x_g, y_g = x_grid.flatten(), y_grid.flatten()
  x_g, y_g = x_g.reshape((len(x_g), 1)), y_g.reshape((len(y_g), 1))
  grid = np.hstack((x_g, y_g))
  p_pred,_,_,_ = function(X_train,Y_train,grid)
  pp_grid = np.array(p_pred).reshape(x_grid.shape)

  # plot the grid of x, y and z values as a surface
  surface = plt.contourf(x_grid, y_grid, pp_grid, colors = ['salmon','greenyellow'])
  # create scatter plot for samples from each class
  row_ix = np.where(Y_test == 1)
  # create scatter of these samples
  plt.scatter(X_test[row_ix, 0], X_test[row_ix, 1], c='g')
  # create scatter plot for samples from each class
  row_ix = np.where(Y_test == -1)
  # create scatter of these samples
  plt.scatter(X_test[row_ix, 0], X_test[row_ix, 1], c='r')
  plt.title('Case'+case)
  plt.show()

classify(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_A,'A')
classify(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_B,'B')
classify(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_C,'C')
classify(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_A,'A')
classify(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_B,'B')
classify(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_C,'C')

"""####4b) Give the ROC Curves for all the classifiers.


"""

# Cell type : CodeWrite
# write the code for loading the data, running the three algos, and plotting here. 
# (Use the functions written previously.)
def truepositives(arr1,arr2):
  count = 0
  for i in range(len(arr1)):
    if arr1[i]==1 and arr1[i]==arr2[i]:
      count+=1
  return count

def truenegatives(arr1,arr2):
  count = 0
  for i in range(len(arr1)):
    if arr1[i]==-1 and arr1[i]==arr2[i]:
      count+=1
  return count

def falsepositives(arr1,arr2):
  count = 0
  for i in range(len(arr1)):
    if arr1[i]==-1 and arr1[i]!=arr2[i]:
      count+=1
  return count

def falsenegatives(arr1,arr2):
  count = 0
  for i in range(len(arr1)):
    if arr1[i]==1 and arr1[i]!=arr2[i]:
      count+=1
  return count

def roc_curve(X_train, Y_train, X_test,Y_test,cl,case):
  n=100
  fpr,tpr = [],[]
  thresholds = np.linspace(0,1,n,endpoint=False)
  for th in thresholds:
    Y_pred_test_1a,_,_,pa = cl(X_train, Y_train, X_test)
    predictions =[]
    boolean = (np.array(pa)>=th)
    for i in boolean:
      if i:
        predictions.append(1)
      else:
        predictions.append(-1)
    fpr.append(falsepositives(Y_test,predictions)/(falsepositives(Y_test,predictions)+truenegatives(Y_test,predictions)))
    tpr.append(truepositives(Y_test,predictions)/(truepositives(Y_test,predictions)+falsenegatives(Y_test,predictions)))
  plt.plot(fpr,tpr)
  plt.xlabel('FPR')
  plt.ylabel('TPR')
  plt.title(case)
  plt.show()

  err_rate = (len(Y_test)-np.sum(np.array(Y_pred_test_1a)==Y_test))/len(Y_test)
  return err_rate

err1 = roc_curve(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_A,'BayesA')
err2 = roc_curve(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_B,'BayesB')
err3 = roc_curve(datasetA['arr_0'],datasetA['arr_1'],datasetA['arr_2'],datasetA['arr_3'],function_for_C,'BayesC')
err4 = roc_curve(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_A,'BayesA')
err5 = roc_curve(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_B,'BayesB')
err6 = roc_curve(datasetB['arr_0'],datasetB['arr_1'],datasetB['arr_2'],datasetB['arr_3'],function_for_C,'BayesC')

"""####4c) In the next Textwrite cell, give the error rate of the three classifiers on the three datasets as 3x2 table, with appropriately named rows and columns.

**Cell type : TextWrite**
(Write your observations and table of errors here)
## TABLE OF ERROR RATES:
>$
\def\arraystretch{1.3}
\begin{array}{@{}l|ll@{}}
Classifier/Dataset & A & B  \\
\hline
BayesA & 0.066  &  0.5085 \\
BayesB &  0.1565    &  0.5065 \\
BayesC &  0.081    &  0.1835 \\
\end{array}$

####4d) In the next Textwrite cell, summarise your observations regarding the 6 learnt classifiers.

**Cell type : TextWrite**
(Write your observations and table of errors here)
## Observations: 

We observe that both the datasets are classified better by the ’BayesC’ classifier.They have smaller error rates compared to the other classifiers.  This is because the classspecific covariance matrix calculation gives more accurate decision boundaries instead of awhole data covariance matrix.We also observe that as the data of different classes overlap by lying in same regions the classifiers ’BayesA’ and ’BayesB’ perform poor (error rates around 0.5) while ’BayesC’ hasa very good performance.
"""