{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "magr_iGPeK-9",
    "outputId": "f6e056e8-3cca-4ae7-9211-b181c58b72c5"
   },
   "source": [
    "### BE18B006_BE18B011 Data Contest Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/gdrive')\n",
    "#%cd /content/gdrive/MyDrive/Colab\\ Notebooks/PRML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-lzl9QiMefCN"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score, confusion_matrix, classification_report, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.feature_selection import f_classif, SelectPercentile, chi2, SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL8RgBOD8JTi"
   },
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wOFosNJu8I9k"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Dataset_1_Training.csv')\n",
    "colnames = df1['ID_REF']\n",
    "df1 = df1.transpose().reset_index().rename(columns = colnames)\n",
    "df1 = df1.drop(index = 0)\n",
    "df1 = df1.rename(columns={'index': 'ID_REF'})\n",
    "df1['CO: 1'], df1['CO: 2'] = df1['CO: 1'].astype(int), df1['CO: 2'].astype(int)\n",
    "\n",
    "X, y1, y2 = df1.iloc[:, 1:-2], df1.iloc[:, -2], df1.iloc[:, -1]\n",
    "\n",
    "# Train test split\n",
    "#df1 = df1.T.drop(['ID_REF'],axis=0)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size = 0.15, random_state = 30)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size = 0.15, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "UzmR400r8TJr"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Dataset_2_Training.csv')\n",
    "colnames = df2['ID_REF']\n",
    "df2 = df2.transpose().reset_index().rename(columns = colnames)\n",
    "df2 = df2.drop(index = 0)\n",
    "df2 = df2.rename(columns={'index': 'ID_REF'})\n",
    "df2['CO: 3'], df2['CO: 4'] = df2['CO: 3'].astype(int), df2['CO: 4'].astype(int)\n",
    "df2['CO: 5'], df2['CO: 6'] = df2['CO: 5'].astype(int), df2['CO: 6'].astype(int)\n",
    "\n",
    "X2, y3, y4, y5, y6 = df2.iloc[:, 1:-4], df2.iloc[:, -4], df2.iloc[:, -3], df2.iloc[:, -2], df2.iloc[:, -1]\n",
    "\n",
    "# Train test split\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y3, test_size = 0.15, random_state = 30)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X2, y4, test_size = 0.15, random_state = 30)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X2, y5, test_size = 0.15, random_state = 30)\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X2, y6, test_size = 0.15, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "P0IvI1YQ8Uwy"
   },
   "outputs": [],
   "source": [
    "#Random forest based feature selection for descriptor1\n",
    "model_RF1 = RandomForestClassifier(n_estimators = 100, n_jobs=-1, random_state=42)\n",
    "model_RF1.fit(X_train1, y_train1)\n",
    "\n",
    "features1 = X.columns\n",
    "importances1 = model_RF1.feature_importances_\n",
    "indices1 = np.argsort(importances1)\n",
    "#Random forest based feature selection for descriptor2\n",
    "model_RF2 = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state = 42)\n",
    "model_RF2.fit(X_train2, y_train2)\n",
    "\n",
    "features2 = X.columns\n",
    "importances2 = model_RF2.feature_importances_\n",
    "indices2 = np.argsort(importances2)\n",
    "#Random forest based feature selection for descriptor3\n",
    "model_RF3 = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state = 42)\n",
    "model_RF3.fit(X_train3, y_train3)\n",
    "\n",
    "features3 = X2.columns\n",
    "importances3 = model_RF3.feature_importances_\n",
    "indices3 = np.argsort(importances3)\n",
    "#Random forest based feature selection for descriptor4\n",
    "model_RF4 = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state = 42)\n",
    "model_RF4.fit(X_train4, y_train4)\n",
    "\n",
    "features4 = X2.columns\n",
    "importances4 = model_RF4.feature_importances_\n",
    "indices4 = np.argsort(importances4)\n",
    "#Random forest based feature selection for descriptor5\n",
    "model_RF5 = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state = 42)\n",
    "model_RF5.fit(X_train5, y_train5)\n",
    "\n",
    "features5 = X2.columns\n",
    "importances5 = model_RF5.feature_importances_\n",
    "indices5 = np.argsort(importances5)\n",
    "#Random forest based feature selection for descriptor6\n",
    "model_RF6 = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state = 42)\n",
    "model_RF6.fit(X_train6, y_train6)\n",
    "\n",
    "features6 = X2.columns\n",
    "importances6 = model_RF6.feature_importances_\n",
    "indices6 = np.argsort(importances6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fejEmG3B8m9M"
   },
   "outputs": [],
   "source": [
    "list1 = list(reversed([features1[i] for i in indices1[-709:]]))\n",
    "list2 = list(reversed([features2[i] for i in indices2[-860:]]))\n",
    "list3 = list(reversed([features3[i] for i in indices3[-1875:]]))\n",
    "list4 = list(reversed([features4[i] for i in indices4[-1497:]]))\n",
    "list5 = list(reversed([features5[i] for i in indices5[-1944:]]))\n",
    "list6 = list(reversed([features6[i] for i in indices6[-2254:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnO8x1hw8qzs"
   },
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ty29OJIBeh3R"
   },
   "outputs": [],
   "source": [
    "train_data_1 = pd.read_csv('Dataset_1_Training.csv', index_col=0).T\n",
    "train_data_2 = pd.read_csv('Dataset_2_Training.csv', index_col=0).T\n",
    "test_data_1 = pd.read_csv('Dataset_1_Testing.csv', index_col=0).T\n",
    "test_data_2 = pd.read_csv('Dataset_2_Testing.csv', index_col=0).T\n",
    "\n",
    "X_train_1 = train_data_1.drop(columns=['CO: 1','CO: 2'])\n",
    "Y_train_CO1 = train_data_1['CO: 1']\n",
    "Y_train_CO2 = train_data_1['CO: 2']\n",
    "\n",
    "X_train_2 = train_data_2.drop(columns=['CO: 3','CO: 4','CO: 5','CO: 6'])\n",
    "Y_train_CO3 = train_data_2['CO: 3']\n",
    "Y_train_CO4 = train_data_2['CO: 4']\n",
    "Y_train_CO5 = train_data_2['CO: 5']\n",
    "Y_train_CO6 = train_data_2['CO: 6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "n9RLRmxWkq5D"
   },
   "outputs": [],
   "source": [
    "def feature_selection(train_df, gene_list):\n",
    "    '''return X_train_CO'''\n",
    "    train_co = train_df.drop(columns=[col for col in train_df.columns.values if col not in gene_list])\n",
    "    return train_co\n",
    "\n",
    "X_train_CO1 = feature_selection(X_train_1, list1)\n",
    "X_train_CO2 = feature_selection(X_train_1, list2)\n",
    "X_train_CO3 = feature_selection(X_train_2, list3)\n",
    "X_train_CO4 = feature_selection(X_train_2, list4)\n",
    "X_train_CO5 = feature_selection(X_train_2, list5)\n",
    "X_train_CO6 = feature_selection(X_train_2, list6)\n",
    "\n",
    "X_test_CO1 = feature_selection(test_data_1,list1)\n",
    "X_test_CO2 = feature_selection(test_data_1,list2)\n",
    "X_test_CO3 = feature_selection(test_data_2,list3)\n",
    "X_test_CO4 = feature_selection(test_data_2,list4)\n",
    "X_test_CO5 = feature_selection(test_data_2,list5)\n",
    "X_test_CO6 = feature_selection(test_data_2,list6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYIHgPDWT_yD",
    "outputId": "e28ad0d7-f86a-4a4d-e280-9f9c519a25a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#final classification model\n",
    "\n",
    "cl2 = LogisticRegression()\n",
    "cl1 = AdaBoostClassifier(n_estimators=10)\n",
    "cl3 = KNeighborsClassifier(n_neighbors=10)\n",
    "#c4 = KNeighborsClassifier(n_neighbors=3)\n",
    "en_1 = make_pipeline(StandardScaler(), VotingClassifier(estimators = [('cl1', cl1),('cl2', cl2),('cl3', cl3)]))\n",
    "en_1.fit(X_train_CO1, Y_train_CO1)\n",
    "y_pred_CO1 = en_1.predict(X_test_CO1)\n",
    "y_pred_train_CO1 = en_1.predict(X_train_CO1)\n",
    "\n",
    "cl2 = LogisticRegression()\n",
    "cl1 = AdaBoostClassifier(n_estimators=25)\n",
    "cl3 = KNeighborsClassifier(n_neighbors=10)\n",
    "#c4 = RandomForestClassifier(max_depth=10)\n",
    "en_2 = make_pipeline(StandardScaler(), PCA(0.95), VotingClassifier(estimators = [('cl1', cl1),('cl2', cl2),('cl3', cl3)]))\n",
    "en_2.fit(X_train_1, Y_train_CO2)\n",
    "y_pred_CO2 = en_2.predict(test_data_1)\n",
    "y_pred_train_CO2 = en_2.predict(X_train_1)\n",
    "\n",
    "cl1 = SVC(gamma='auto', probability=True)\n",
    "cl2 = BaggingClassifier(LogisticRegression(), max_samples = 0.5, max_features=0.5)\n",
    "cl3 = RandomForestClassifier(n_estimators=5)\n",
    "#c4 = KNeighborsClassifier(n_neighbors=50)\n",
    "en_3 = make_pipeline(StandardScaler(), VotingClassifier(estimators = [('cl1', cl1),('cl2', cl2),('cl3', cl3)]))\n",
    "en_3.fit(X_train_CO3, Y_train_CO3)\n",
    "y_pred_CO3 = en_3.predict(X_test_CO3)\n",
    "y_pred_train_CO3 = en_3.predict(X_train_CO3)\n",
    "\n",
    "cl1 = BaggingClassifier(KNeighborsClassifier(), max_samples = 0.5, max_features=0.5)\n",
    "cl2 = AdaBoostClassifier(n_estimators=50)\n",
    "cl3 = RandomForestClassifier(n_estimators=5)\n",
    "en_4 = make_pipeline(StandardScaler(), VotingClassifier(estimators = [('cl1', cl1),('cl2', cl2),('cl3', cl3)]))\n",
    "en_4.fit(X_train_2, Y_train_CO4)\n",
    "y_pred_CO4 = en_4.predict(test_data_2)\n",
    "y_pred_train_CO4 = en_4.predict(X_train_2)\n",
    "\n",
    "en_5 = make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=20))\n",
    "en_5.fit(X_train_CO5, Y_train_CO5)\n",
    "y_pred_CO5 = en_5.predict(X_test_CO5)\n",
    "y_pred_train_CO5 = en_5.predict(X_train_CO5)\n",
    "\n",
    "#c4 = BaggingClassifier(LogisticRegression(), max_samples = 0.5, max_features=0.5)\n",
    "cl1 = GradientBoostingClassifier(n_estimators=100)\n",
    "cl2 = RandomForestClassifier(n_estimators=10)\n",
    "cl3 = RandomForestClassifier(n_estimators=100)\n",
    "en_6 = make_pipeline(StandardScaler(), PCA(0.95), VotingClassifier(estimators = [('cl1', cl1),('cl2', cl2),('cl3', cl3)]))\n",
    "en_6.fit(X_train_2, Y_train_CO6)\n",
    "y_pred_CO6 = en_6.predict(test_data_2)\n",
    "y_pred_train_CO6 = en_6.predict(X_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "msAB1alwCkws"
   },
   "outputs": [],
   "source": [
    "result = np.concatenate((y_pred_CO1,y_pred_CO2,y_pred_CO3,y_pred_CO4,y_pred_CO5,y_pred_CO6), axis=0)\n",
    "answer = pd.DataFrame([int(i) for i in range(len(result))], columns = ['Id'])\n",
    "answer['Predicted'] = pd.DataFrame(result)\n",
    "answer['Predicted'] = answer['Predicted'].astype(int)\n",
    "\n",
    "# Write CSV file\n",
    "answer.to_csv('BE18B006_BE18B011_result.csv', encoding='utf-8', mode='a', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BE18B006_BE18B011.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
